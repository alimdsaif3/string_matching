# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, f1_score

# Step 1: Data Preprocessing
# Load a dataset containing pairs of strings or text data (replace 'your_dataset.csv' with your data)
data = pd.read_csv('your_dataset.csv')

# Data cleaning and preprocessing (you may need more preprocessing steps)
for col in data.columns:
    if data[col].dtype == 'object':
        data[col] = data[col].str.lower()

# Split the dataset into a training set and a validation/test set
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

# Step 2: Feature Engineering
# Extract relevant features using TF-IDF vectorization for all text columns
text_columns = [col for col in data.columns if data[col].dtype == 'object']

tfidf_vectorizers = {}
for col in text_columns:
    tfidf_vectorizers[col] = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizers[col].fit_transform(train_data[col])
    train_data[col] = list(tfidf_matrix)

# Step 3: Model Building
# Create an XGBoost model with L2 regularization (ridge)
xgb_model = XGBClassifier(reg_alpha=1.0)

# Step 4: Model Training
# Combine TF-IDF matrices for all text columns into a single input matrix
X_train = np.hstack([train_data[col] for col in text_columns])
y_train = train_data['label']

xgb_model.fit(X_train, y_train)

# Step 5: Model Evaluation
# Transform the test data using the same vectorizer
for col in text_columns:
    test_tfidf_matrix = tfidf_vectorizers[col].transform(test_data[col])
    test_data[col] = list(test_tfidf_matrix)

# Combine TF-IDF matrices for all text columns into a single input matrix
X_test = np.hstack([test_data[col] for col in text_columns])
y_test = test_data['label']

# Make predictions
predictions = xgb_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, predictions)
f1 = f1_score(y_test, predictions)

print("Accuracy: {:.2f}%".format(accuracy * 100))
print("F1 Score: {:.2f}".format(f1))

# Step 6: Fine-Tuning
# You can experiment with different hyperparameters to optimize the model.
# For example, you can adjust the learning rate, max depth, and regularization strength.

xgb_model = XGBClassifier(
    learning_rate=0.1,  # Adjust the learning rate as needed
    max_depth=5,  # Adjust the depth of the trees
    reg_alpha=0.01  # Adjust the L2 regularization strength
)

# Re-train the model with the updated hyperparameters
xgb_model.fit(X_train, y_train)

# Step 7: Prediction
# You can use the trained model to make predictions on new data.

# Load new data for prediction (replace 'new_data.csv' with your data)
new_data = pd.read_csv('new_data.csv')

# Preprocess the new data (use the same preprocessing steps as earlier)

# Combine TF-IDF matrices for all text columns into a single input matrix
for col in text_columns:
    new_tfidf_matrix = tfidf_vectorizers[col].transform(new_data[col])
    new_data[col] = list(new_tfidf_matrix)

X_new = np.hstack([new_data[col] for col in text_columns])

# Make predictions on the new data
new_predictions = xgb_model.predict(X_new)

# Step 8: Reporting
# Report the model's performance on the new data by saving the predictions, calculating evaluation metrics,
# and generating any necessary visualizations. You can also report the evaluation metrics for the test data.

# Save the new predictions to a CSV file
new_data['predictions'] = new_predictions
new_data.to_csv('predictions_new_data.csv', index=False)

# Optionally, you can save the trained model for future use
# xgb_model.save_model("string_matching_model.model")


